{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow lite\n",
    "\n",
    "Tensorflow Lite is a set of tools for running TensorFlow models on edge devices. It has two main components:\n",
    "\n",
    "- TensorFlow Lite Converter: This converts TF models into special efficient models for use in memory-contrained devices. It can reduce the model's size and make it run faster on edge devices.\n",
    "\n",
    "- Tensorfow Lite Interpreter: This runs efficiently TensorFlow Lite models.\n",
    "\n",
    "We will convert our previous saved models using the TensorFlow Converter's Python API. We will perform quantization to a model, one of the most well known optimitzations (and also needed to run our model in a Coral device, for example).\n",
    "\n",
    "You can find more information in the official [TensorFlow Lite documentation](https://www.tensorflow.org/api_docs/python/tf/lite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # to display images\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About quantization\n",
    "\n",
    "By default, the model parameters are stored as 32-bit floating-point numbers. Quantization is a technique that allows us to reduce the precision of these parameters to 8-bit integers. This will produce a 4 times reduction in size and will also increase the speed of inference because it is easier for the CPU to operate with integers.\n",
    "\n",
    "The quantization process requires a representative dataset that must represent the full range of input values. This is required to adjust the dynamic range of the quantization levels.\n",
    "\n",
    "<center><img src=\"assets/quantization.png\" alt=\"quantization\" width=\"600\"/></center>\n",
    "\n",
    "To use our model with TensorFlow Lite, wee need to convert it first. In order to do this, we will use the **TensorFlow Lite Converter's Python API**. Using this API we can write the model in the form of a **FlatBuffer**, which is more space-efficient. It also can apply several optimizations to the model, to reduce its size, the time of inferences or both.\n",
    "\n",
    "These optimizations can cost a reduction on the accuracy of the model, but they are usually very small, so applying these optimizations is preferred. In order to compare the effect of quantization, we will deploy two models, one without quantization and the other with quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the MINST dataset again to perform the quantization optimization.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a generator function that provides our test data's X values as a representative dataset \n",
    "# and tell the converter to use it\n",
    "def representative_dataset_generator():\n",
    "    for value in x_train:\n",
    "        yield [value.reshape(1, 28, 28, 1).astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the target baseline model\n",
    "model_4 = keras.models.load_model(\"Models/model_4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model conversion 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to the Tensorflow Lite format without quantitzation\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_4)\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "# Save the model to disk\n",
    "open(\"Models/converted_model_4.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model conversion 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_4)\n",
    "# This enables quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# This sets the representative dataset for quantization\n",
    "converter.representative_dataset = representative_dataset_generator\n",
    "# This ensures that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "# These set the input and output tensors to uint8 (added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "# Convert the model\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"Models/converted_quant_model_4.tflite\", \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions:np.ndarray, ground_truth:np.ndarray):\n",
    "\n",
    "    assert len(predictions) == len(ground_truth)\n",
    "    \n",
    "    acc = 0\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        if np.argmax(predictions[i]) == ground_truth[i]:\n",
    "            acc += 1\n",
    "\n",
    "    return acc/len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed = 0\n",
    "predictions_norm = []\n",
    "\n",
    "for x_value in tqdm(x_test):\n",
    "\n",
    "    x_value = x_value.reshape(1, 28, 28, 1)\n",
    "    tic = time.perf_counter()\n",
    "    prediction = model_4.predict(x_value)\n",
    "    elapsed += time.perf_counter() - tic\n",
    "    predictions_norm.append(prediction)\n",
    "\n",
    "tf_time = elapsed/len(x_test)\n",
    "tf_acc = compute_accuracy(predictions=predictions_norm, ground_truth=y_test)\n",
    "tf_size = os.path.getsize(\"Models/model_4.h5\")\n",
    "\n",
    "print(\"TF elapsed time/inference(s): \", tf_time)\n",
    "print(\"TF acc: \", tf_acc)\n",
    "print(\"TF model size (bytes): \", tf_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converted model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an interpreter\n",
    "tflite_model = tf.lite.Interpreter(\"Models/converted_model_4.tflite\")\n",
    "# Allocate memory for each model\n",
    "tflite_model.allocate_tensors()\n",
    "# Get the indexes of the input and output tensors\n",
    "tflite_model_input_index = tflite_model.get_input_details()[0]['index']\n",
    "tflite_model_output_index = tflite_model.get_output_details()[0]['index']\n",
    "# Create arrays to store the results\n",
    "predictions_tflite = []\n",
    "elapsed = 0\n",
    "\n",
    "for x_value in tqdm(x_test):\n",
    "\n",
    "    x_value = x_value.reshape(1, 28, 28, 1).astype(np.float32)\n",
    "    # Create a tensor wrapping the current x value\n",
    "    x_value_tensor = tf.convert_to_tensor(x_value, dtype = np.float32)\n",
    "    # Write the value to the input tensor\n",
    "    tflite_model.set_tensor(tflite_model_input_index, x_value_tensor)\n",
    "    # Run inference\n",
    "    tic = time.perf_counter()\n",
    "    tflite_model.invoke()\n",
    "    elapsed += time.perf_counter() - tic\n",
    "    # Read the prediction and store it\n",
    "    predictions_tflite.append(tflite_model.get_tensor(tflite_model_output_index)[0])\n",
    "\n",
    "tflite_time = elapsed/len(x_test)\n",
    "tflite_acc = compute_accuracy(predictions=predictions_tflite, ground_truth=y_test)\n",
    "tflite_size = os.path.getsize(\"Models/converted_model_4.tflite\")\n",
    "\n",
    "print(\"TFLite elapsed time/inference(s): \", tflite_time)\n",
    "print(\"TFLite acc: \", tflite_acc)\n",
    "print(\"TFLite model size (bytes): \", tflite_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converted and quantizated model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input_tensor(interpreter, input):\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  tensor_index = input_details['index']\n",
    "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "  # Inputs for the TFLite model must be uint8, so we quantize our input data.\n",
    "  # NOTE: This step is necessary only because we're receiving input data from\n",
    "  # ImageDataGenerator, which rescaled all image data to float [0,1]. When using\n",
    "  # bitmap inputs, they're already uint8 [0,255] so this can be replaced with:\n",
    "  #   input_tensor[:, :] = input\n",
    "  scale, zero_point = input_details['quantization']\n",
    "  input_tensor[:, :] = np.uint8(input / scale + zero_point)\n",
    "\n",
    "def classify_image(interpreter, input):\n",
    "  set_input_tensor(interpreter, input)\n",
    "\n",
    "  tic = time.perf_counter()\n",
    "  interpreter.invoke()\n",
    "  elapsed = time.perf_counter() - tic\n",
    "\n",
    "  # Get the indexes of the input and output tensors\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "  output = interpreter.get_tensor(output_details['index'])\n",
    "  # Outputs from the TFLite model are uint8, so we dequantize the results:\n",
    "  scale, zero_point = output_details['quantization']\n",
    "  output = scale * (output - zero_point)\n",
    "\n",
    "  return elapsed,output\n",
    "\n",
    "# Instantiate an interpreter\n",
    "quant_model = tf.lite.Interpreter(\"Models/converted_quant_model_4.tflite\")\n",
    "# Allocate memory for each model\n",
    "quant_model.allocate_tensors()\n",
    "\n",
    "# Create arrays to store the results\n",
    "predictions_quant = []\n",
    "elapsed = 0\n",
    "\n",
    "for x_value in tqdm(x_test):\n",
    "\n",
    "    pred_time, pred = classify_image(quant_model, x_value.reshape(1,28,28,1))\n",
    "    predictions_quant.append(pred)\n",
    "    elapsed += pred_time\n",
    "\n",
    "tflite_quant_time = elapsed/len(x_test)\n",
    "tflite_quant_acc = compute_accuracy(predictions=predictions_quant, ground_truth=y_test)\n",
    "tflite_quant_size = os.path.getsize(\"Models/converted_quant_model_4.tflite\")\n",
    "\n",
    "print(\"TFLite quant elapsed time/inference(s): \", tflite_quant_time)\n",
    "print(\"TFLite quant acc: \", tflite_quant_acc)\n",
    "print(\"TFLite quant model size (bytes): \", tflite_quant_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"name\" : [\"regular TF\", \"TF lite\", \"TF lite quant\"],\n",
    "    \"time/inference(s)\" : [tf_time, tflite_time, tflite_quant_time],\n",
    "    \"size(bytes)\" : [tf_size, tflite_size, tflite_quant_size],\n",
    "    \"acc\" : [tf_acc, tflite_acc, tflite_quant_acc]   \n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e54000f159efabab77c845bbe9e2981a9b2968a0871fc06e746fcfb112344f88"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
